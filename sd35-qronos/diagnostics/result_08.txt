(bacp) C:\Users\Fatim_Sproj\Desktop\Fatim\edge\Project\AI624-Diffusion-Quantization\sd35-qronos>python scripts/diag_08_inspect_model.py                                                                                                         C:\Users\Fatim_Sproj\anaconda3\envs\bacp\lib\site-packages\torch\cuda\__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.                                                                   import pynvml  # type: ignore[import]                                                                                 2025-12-21 04:08:24.225003: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.                                                                   2025-12-21 04:08:25.045654: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.                                                                   ============================================================                                                            INSPECT ACTUAL QUANTIZED MODEL                                                                                          ============================================================                                                                                                                                                                                    [1/3] Loading FP16 baseline...                                                                                          Loading SD 3.5 Medium from stabilityai/stable-diffusion-3.5-medium...                                                   Loading pipeline components...:   0%|                                                            | 0/9 [00:00<?, ?it/s]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers                                 Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.81it/s] Loading pipeline components...: 100%|████████████████████████████████████████████████████| 9/9 [00:02<00:00,  3.91it/s] Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.                                                                                           Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.                                                                                           Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.                                                                                           Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.                                                                                           Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.                                                                                           Pipeline loaded successfully                                                                                              Transformer: 4.60 GB                                                                                                    Device: cpu                                                                                                                                                                                                                                   [2/3] Loading quantized model...                                                                                        Loading SD 3.5 Medium from stabilityai/stable-diffusion-3.5-medium...                                                   Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10.57it/s] Loading pipeline components...: 100%|████████████████████████████████████████████████████| 9/9 [00:02<00:00,  4.04it/s] Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.                                                                                           Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.                                                                                           Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.                                                                                           Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.                                                                                           Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.                                                                                           Pipeline loaded successfully                                                                                              Transformer: 4.60 GB                                                                                                    Device: cpu                                                                                                                                                                                                                                   [3/3] Comparing weights...                                                                                                                                                                                                                      ============================================================                                                            FINDINGS                                                                                                                ============================================================                                                                                                                                                                                    Total layers checked: 515                                                                                                                                                                                                                       ✅ No NaN values found                                                                                                  ✅ No Inf values found                                                                                                  ✅ No all-zero layers                                                                                                                                                                                                                           ⚠️ LAYERS WITH >50% CHANGE: 9                                                                                              - transformer_blocks.21.ff_context.net.0.proj.weight: 304.1% change                                                     - transformer_blocks.22.ff_context.net.0.proj.weight: 196.3% change                                                     - transformer_blocks.20.ff_context.net.0.proj.weight: 155.3% change                                                     - transformer_blocks.1.attn.to_q.weight: 139.9% change                                                                  - transformer_blocks.1.attn2.to_q.weight: 135.3% change                                                                 - transformer_blocks.15.ff_context.net.2.weight: 112.4% change                                                          - transformer_blocks.18.ff_context.net.2.weight: 63.2% change                                                           - transformer_blocks.1.ff.net.0.proj.weight: 55.6% change                                                               - transformer_blocks.15.ff_context.net.0.proj.weight: 52.6% change                                                                                                                                                                           ------------------------------------------------------------                                                            OVERALL STATISTICS                                                                                                      ------------------------------------------------------------                                                            Mean relative change: 12.04%                                                                                            Max relative change: 304.08%                                                                                            Median relative change: 0.00%                                                                                                                                                                                                                   Unchanged layers (not quantized): 300                                                                                                                                                                                                           Full results saved to diagnosis_inspect_model                                                                                                                                                                                                   ============================================================                                                            VERDICT                                                                                                                 ============================================================                                                            ✅ Weight statistics look reasonable                                                                                       Issue may be elsewhere (e.g., layer interactions)                                                                                                                                                                                            (bacp) C:\Users\Fatim_Sproj\Desktop\Fatim\edge\Project\AI624-Diffusion-Quantization\sd35-qronos> 